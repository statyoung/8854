{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.124316239289707\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 10000\n",
    "np.random.seed(42)\n",
    "\n",
    "x0 = np.random.exponential(scale=1.0, size=N)\n",
    "x1 = np.random.lognormal(mean=0.0, sigma=1.0, size=N)\n",
    "x2 = np.random.normal(loc=0.0, scale=1.0, size=N)\n",
    "x3 = np.random.normal(loc=0.0, scale=1.0, size=N)\n",
    "x4 = np.random.normal(loc=0.0, scale=1.0, size=N)\n",
    "\n",
    "# Add strong outliers to 1% of x2\n",
    "outlier_idx = np.random.choice(N, size=int(0.01 * N), replace=False)\n",
    "x2[outlier_idx] *= 10.0  # Amplify outliers\n",
    "\n",
    "# Uniform distribution for stable features\n",
    "x5 = np.random.uniform(-1, 1, size=N)\n",
    "x6 = np.random.uniform(-1, 1, size=N)\n",
    "x7 = np.random.uniform(-1, 1, size=N)\n",
    "x8 = np.random.uniform(-1, 1, size=N)\n",
    "x9 = np.random.uniform(-1, 1, size=N)\n",
    "\n",
    "# 2. Define target function (nonlinear + noise)\n",
    "def custom_f(x0, x1, x2, x3, x4):\n",
    "    return (\n",
    "        2.0 * np.log1p(x0)    # log(1 + x0) to handle long-tail\n",
    "        + 0.5 * np.sqrt(x1)   # sqrt(x1) to smooth lognormal\n",
    "        - 3.0 * np.cos(x2)    # periodic effect\n",
    "        + 2.0 * x3**2         # quadratic term\n",
    "        + np.tanh(x4)         # nonlinearity\n",
    "    )\n",
    "\n",
    "noise = np.random.normal(loc=0, scale=1.0, size=N)\n",
    "y = custom_f(x0, x1, x2, x3, x4) + noise\n",
    "\n",
    "# 3. Convert to DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'x0': x0, 'x1': x1, 'x2': x2, 'x3': x3, 'x4': x4,\n",
    "    'x5': x5, 'x6': x6, 'x7': x7, 'x8': x8, 'x9': x9,\n",
    "    'y': y\n",
    "})\n",
    "\n",
    "X = df.drop(columns=['y'])\n",
    "y = df['y']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-fill SHAP feature-wise std: [0.00492508 0.0019016  0.09378474 0.1031966  0.0080946  0.00190162\n",
      " 0.00190162 0.00190162 0.00190162 0.00190162]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from math import comb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "test_idx = 0\n",
    "\n",
    "def kernel_shap_no_correction_single_mean_fill(model, X_train, X_test_point):\n",
    "    \"\"\"\n",
    "    Compute Kernel SHAP values using a mean-filled baseline without correction.\n",
    "    \n",
    "    - The reference baseline is the mean vector of X_train.\n",
    "    - SHAP values are estimated via weighted linear regression.\n",
    "    \"\"\"\n",
    "    p = X_train.shape[1]\n",
    "    mean_vector = np.mean(X_train, axis=0)  # Compute the mean of each feature\n",
    "    baseline = model.predict(mean_vector.reshape(1, -1))[0]  # Predict baseline output\n",
    "\n",
    "    subset_list = []\n",
    "    for size_s in range(1, p):  # Generate all non-empty subsets\n",
    "        for combi in combinations(range(p), size_s):\n",
    "            subset_list.append(combi)\n",
    "    num_subsets = len(subset_list)\n",
    "\n",
    "    # Initialize matrices for regression\n",
    "    bigX = np.zeros((num_subsets, p))\n",
    "    bigy = np.zeros(num_subsets)\n",
    "    sample_weight = np.zeros(num_subsets)\n",
    "\n",
    "    row_idx = 0\n",
    "    for subset in subset_list:\n",
    "        s = len(subset)\n",
    "        w_s = (p - 1) / (comb(p, s) * s * (p - s))  # Weight for subset\n",
    "\n",
    "        mask_vec = np.zeros(p)\n",
    "        mask_vec[list(subset)] = 1.0  # Mark selected features\n",
    "\n",
    "        # Construct perturbed input by copying mean vector\n",
    "        perturbed = mean_vector.copy()\n",
    "        perturbed[list(subset)] = X_test_point[list(subset)]\n",
    "\n",
    "        fx = model.predict(perturbed.reshape(1, -1))[0]  # Get model prediction\n",
    "        \n",
    "        bigy[row_idx] = fx - baseline  # Compute difference from baseline\n",
    "        bigX[row_idx, :] = mask_vec  # Store mask vector\n",
    "        sample_weight[row_idx] = w_s  # Store weight\n",
    "\n",
    "        row_idx += 1\n",
    "\n",
    "    # Fit weighted linear regression\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    lr.fit(bigX, bigy, sample_weight=sample_weight)\n",
    "    shap_values = lr.coef_\n",
    "    return baseline, shap_values\n",
    "\n",
    "\n",
    "x_test_1_arr = X_test.iloc[test_idx].values\n",
    "\n",
    "# Add small perturbations to x_test_1_arr multiple times\n",
    "mean_fill_shap_values_list = []\n",
    "n_runs = 100\n",
    "for seed in range(n_runs):\n",
    "    np.random.seed(seed)\n",
    "    # Add small Gaussian noise to the input\n",
    "    x_test_perturbed = x_test_1_arr + np.random.normal(0, 0.01, size=len(x_test_1_arr))\n",
    "    \n",
    "    baseline, shap_vals = kernel_shap_no_correction_single_mean_fill(\n",
    "        model, \n",
    "        X_train.values, \n",
    "        x_test_perturbed\n",
    "    )\n",
    "    mean_fill_shap_values_list.append(shap_vals)\n",
    "\n",
    "# Compute standard deviation of SHAP values across runs\n",
    "shap_array = np.array(mean_fill_shap_values_list)  # shape=(n_runs, p)\n",
    "stds = shap_array.std(axis=0)\n",
    "print(\"Mean-fill SHAP feature-wise std:\", stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "def tabular_lime_explanation_regression(\n",
    "    X_train: np.ndarray,\n",
    "    model,                 \n",
    "    X_test_point: np.ndarray,  \n",
    "    num_samples: int = 5000,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    LIME-based explanation for tabular regression models:\n",
    "      1) Compute quartiles (bins) for each feature\n",
    "      2) Calculate mean/std for each bin\n",
    "      3) Sample from bins using truncated Gaussian distribution\n",
    "      4) Compute distance-based weights from X_test_point\n",
    "      5) Fit Ridge regression for local interpretation\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    n_train, n_features = X_train.shape\n",
    "    \n",
    "    # 0) StandardScaler for distance calculations\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "\n",
    "    # 1) Compute (min, q1, q2, q3, max) for each feature\n",
    "    bin_edges_list = []\n",
    "    for j in range(n_features):\n",
    "        col_j = X_train[:, j]\n",
    "        min_j = np.min(col_j)\n",
    "        q1_j  = np.percentile(col_j, 25)\n",
    "        q2_j  = np.percentile(col_j, 50)\n",
    "        q3_j  = np.percentile(col_j, 75)\n",
    "        max_j = np.max(col_j)\n",
    "        bin_edges_list.append([min_j, q1_j, q2_j, q3_j, max_j])\n",
    "    bin_edges = np.array(bin_edges_list)  # shape=(n_features, 5)\n",
    "\n",
    "    # 2) Compute mean, std, and count for each bin\n",
    "    means = np.zeros((n_features, 4))\n",
    "    stds  = np.zeros((n_features, 4))\n",
    "    counts = np.zeros((n_features, 4), dtype=int)\n",
    "\n",
    "    for j in range(n_features):\n",
    "        edges_j = bin_edges[j]  # [min, q1, q2, q3, max]\n",
    "        col_j = X_train[:, j]\n",
    "\n",
    "        for i_bin in range(4):\n",
    "            left  = edges_j[i_bin]\n",
    "            right = edges_j[i_bin+1]\n",
    "            if i_bin == 0:\n",
    "                mask = (col_j >= left) & (col_j <= right)\n",
    "            else:\n",
    "                mask = (col_j > left) & (col_j <= right)\n",
    "\n",
    "            bin_data = col_j[mask]\n",
    "            counts[j, i_bin] = len(bin_data)\n",
    "            if len(bin_data) == 0:\n",
    "                # If bin is empty, assign fallback values\n",
    "                means[j, i_bin] = 0.5 * (left + right)\n",
    "                stds[j, i_bin]  = max(1e-6, (right - left) / 6)\n",
    "            else:\n",
    "                means[j, i_bin] = np.mean(bin_data)\n",
    "                tmp_std = np.std(bin_data, ddof=1)\n",
    "                stds[j, i_bin]  = tmp_std if tmp_std >= 1e-9 else 1e-9\n",
    "\n",
    "    # Compute bin selection probability (based on sample count)\n",
    "    probs = counts / float(n_train)  # shape=(n_features, 4)\n",
    "\n",
    "    # 3) Generate perturbed samples\n",
    "    X_samples = np.zeros((num_samples, n_features))\n",
    "    for i_smp in range(num_samples):\n",
    "        for j in range(n_features):\n",
    "            p_j = probs[j]\n",
    "            bin_idx = rng.choice(4, p=p_j)  # Select bin\n",
    "\n",
    "            m = means[j, bin_idx]\n",
    "            s = stds[j, bin_idx]\n",
    "            left  = bin_edges[j, bin_idx]\n",
    "            right = bin_edges[j, bin_idx+1]\n",
    "\n",
    "            if counts[j, bin_idx] == 0:\n",
    "                # If bin is empty, sample from uniform distribution\n",
    "                val = rng.uniform(left, right)\n",
    "            else:\n",
    "                # Sample from truncated normal distribution\n",
    "                a = (left - m) / s\n",
    "                b = (right - m) / s\n",
    "                val = truncnorm.rvs(a, b, loc=m, scale=s, random_state=rng)\n",
    "\n",
    "            X_samples[i_smp, j] = val\n",
    "\n",
    "    # 4) Compute distance-based weights\n",
    "    #    - Compute L2 distance in original scale\n",
    "    dists = pairwise_distances(X_samples, X_test_point.reshape(1, -1),\n",
    "                               metric='euclidean').ravel()\n",
    "    kernel_width = 0.75 * np.sqrt(n_features)\n",
    "    weights = np.exp(-(dists**2) / (kernel_width**2))\n",
    "\n",
    "    # 5) Predict f(perturbed X)\n",
    "    y_smp = model.predict(X_samples)  # shape: (num_samples,)\n",
    "\n",
    "    # Fit Ridge regression for local explanation\n",
    "    ridge = Ridge(alpha=1e-6, fit_intercept=True, random_state=rng)\n",
    "    ridge.fit(X_samples, y_smp, sample_weight=weights)\n",
    "    coefs = ridge.coef_       # shape=(n_features,)\n",
    "    intercept = ridge.intercept_\n",
    "\n",
    "    # Compute local prediction g(x_test)\n",
    "    g_x_test = intercept + coefs.dot(X_test_point)\n",
    "\n",
    "    return coefs, g_x_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== LIME (x_test에 노이즈 추가) ==\n",
      "Feature-wise std: [0.00098194 0.0004485  0.00592275 0.01023281 0.00057645 0.00082646\n",
      " 0.00064816 0.00061897 0.00065979 0.00081536]\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "#  (Experiment) Add small noise to x_test and compare LIME explanations\n",
    "# =========================================\n",
    "\n",
    "# Assume we have a trained XGBoost model with X_train and X_test\n",
    "# test_idx = 0\n",
    "# x_test_1_arr = X_test.iloc[test_idx].values\n",
    "\n",
    "n_runs = 100\n",
    "lime_coefs_list = []\n",
    "\n",
    "for seed in range(n_runs):\n",
    "    np.random.seed(seed)\n",
    "    # Experiment: Add small noise to the test point\n",
    "    x_test_perturbed = x_test_1_arr + np.random.normal(0, 0.01, size=len(x_test_1_arr))\n",
    "    \n",
    "    # Fix the internal sampling random_state (=42) to ensure consistent sampling inside LIME\n",
    "    coefs, g_x_pert = tabular_lime_explanation_regression(\n",
    "        X_train.values,   # shape=(N, p)\n",
    "        model, \n",
    "        x_test_perturbed, \n",
    "        num_samples=5000,  # Adjust as needed\n",
    "        random_state=42\n",
    "    )\n",
    "    lime_coefs_list.append(coefs)\n",
    "\n",
    "# Compute feature-wise standard deviation\n",
    "lime_coefs_array = np.array(lime_coefs_list)  # shape=(n_runs, p)\n",
    "lime_stds = lime_coefs_array.std(axis=0)\n",
    "\n",
    "print(\"== LIME (x_test with added noise) ==\")\n",
    "print(\"Feature-wise std:\", lime_stds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 feature별 SHAP std 합 (길이 p): [1.6353959762579373, 0.4652943055117794, 2.4635063203016876, 3.605101503605942, 1.2044506450431258, 0.1459028839431412, 0.19018452271894926, 0.1624623307187004, 0.11330356081312232, 0.10724681947183715]\n",
      "각 feature별 LIME std 합 (길이 p): [0.06692954801752421, 0.03872961276675696, 0.6135925965690895, 0.8679351210320528, 0.05752777187285849, 0.09016409051313498, 0.08775053013205017, 0.0914356143663165, 0.08199209494140974, 0.08012904055744668]\n"
     ]
    }
   ],
   "source": [
    "N_samples_for_test = 100        # Number of test samples (random selection)\n",
    "n_runs = 10                     # Number of iterations per sample\n",
    "p = X_train.shape[1]            # Number of features\n",
    "\n",
    "# (Results) Arrays to accumulate the sum of standard deviations for each feature\n",
    "shap_std_sum_each_feature = np.zeros(p)  \n",
    "lime_std_sum_each_feature = np.zeros(p)  \n",
    "\n",
    "# Randomly select 100 test samples (without replacement)\n",
    "np.random.seed(42)  # Fix random seed for reproducibility\n",
    "test_indices = np.random.choice(len(X_test), size=N_samples_for_test, replace=False)\n",
    "\n",
    "for idx in test_indices:\n",
    "    # Current test sample\n",
    "    x_test_original = X_test.iloc[idx].values  # shape=(p,)\n",
    "\n",
    "    # Lists to store SHAP and LIME results across runs\n",
    "    shap_values_runs = []\n",
    "    lime_coefs_runs = []\n",
    "\n",
    "    for run_id in range(n_runs):\n",
    "        # Add small noise to x_test_point\n",
    "        x_test_perturbed = x_test_original + np.random.normal(0, 0.01, size=p)\n",
    "\n",
    "        # Compute SHAP values\n",
    "        _, shap_vals = kernel_shap_no_correction_single_mean_fill(\n",
    "            model, \n",
    "            X_train.values, \n",
    "            x_test_perturbed\n",
    "        )\n",
    "        shap_values_runs.append(shap_vals)\n",
    "\n",
    "        # Compute LIME coefficients (random_state fixed)\n",
    "        coefs, _ = tabular_lime_explanation_regression(\n",
    "            X_train.values, \n",
    "            model, \n",
    "            x_test_perturbed,\n",
    "            num_samples=3000,\n",
    "            random_state=42  \n",
    "        )\n",
    "        lime_coefs_runs.append(coefs)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    shap_values_runs = np.array(shap_values_runs)  # shape=(n_runs, p)\n",
    "    lime_coefs_runs = np.array(lime_coefs_runs)    # shape=(n_runs, p)\n",
    "\n",
    "    # Compute feature-wise standard deviation\n",
    "    shap_std = shap_values_runs.std(axis=0)  # shape=(p,)\n",
    "    lime_std = lime_coefs_runs.std(axis=0)   # shape=(p,)\n",
    "\n",
    "    # Accumulate feature-wise standard deviations over 100 samples\n",
    "    shap_std_sum_each_feature += shap_std\n",
    "    lime_std_sum_each_feature += lime_std\n",
    "\n",
    "# At the end, shap_std_sum_each_feature and lime_std_sum_each_feature (shape=(p,))\n",
    "# contain the total sum of std values recorded for each feature across 100 samples.\n",
    "\n",
    "# If Python lists are needed, use .tolist()\n",
    "shap_std_sum_list = shap_std_sum_each_feature.tolist()\n",
    "lime_std_sum_list = lime_std_sum_each_feature.tolist()\n",
    "\n",
    "print(\"각 feature별 SHAP std 합 (길이 p):\", shap_std_sum_list)\n",
    "print(\"각 feature별 LIME std 합 (길이 p):\", lime_std_sum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "각 feature별 SHAP std 합 (길이 p): [1.8443670359038722, 0.5263056628818887, 2.369896623498704, 4.078381324536716, 0.9988604851775671, 0.19575748841733667, 0.1820129666393052, 0.2665861800989916, 0.1690421875241291, 0.12105209826547984]\n",
      "각 feature별 LIME std 합 (길이 p): [0.06471496939269901, 0.045084181017521145, 0.6228025981202929, 0.9518710939304331, 0.05652144688255208, 0.08569474051525917, 0.09064247218334816, 0.09947671189796813, 0.08221566371881076, 0.08119491512117116]\n"
     ]
    }
   ],
   "source": [
    "N_samples_for_test = 100        # Number of test samples (random selection)\n",
    "n_runs = 40                     # Number of iterations per sample\n",
    "p = X_train.shape[1]            # Number of features\n",
    "\n",
    "# (Results) Arrays to accumulate the sum of standard deviations for each feature\n",
    "shap_std_sum_each_feature = np.zeros(p)  \n",
    "lime_std_sum_each_feature = np.zeros(p)  \n",
    "\n",
    "# Randomly select 100 test samples (without replacement)\n",
    "np.random.seed(41)  # Fix random seed for reproducibility\n",
    "test_indices = np.random.choice(len(X_test), size=N_samples_for_test, replace=False)\n",
    "\n",
    "for idx in test_indices:\n",
    "    # Current test sample\n",
    "    x_test_original = X_test.iloc[idx].values  # shape=(p,)\n",
    "\n",
    "    # Lists to store SHAP and LIME results across runs\n",
    "    shap_values_runs = []\n",
    "    lime_coefs_runs = []\n",
    "\n",
    "    for run_id in range(n_runs):\n",
    "        # Add small noise to x_test_point\n",
    "        x_test_perturbed = x_test_original + np.random.normal(0, 0.01, size=p)\n",
    "\n",
    "        # Compute SHAP values\n",
    "        _, shap_vals = kernel_shap_no_correction_single_mean_fill(\n",
    "            model, \n",
    "            X_train.values, \n",
    "            x_test_perturbed\n",
    "        )\n",
    "        shap_values_runs.append(shap_vals)\n",
    "\n",
    "        # Compute LIME coefficients (random_state fixed)\n",
    "        coefs, _ = tabular_lime_explanation_regression(\n",
    "            X_train.values, \n",
    "            model, \n",
    "            x_test_perturbed,\n",
    "            num_samples=3000,\n",
    "            random_state=42  \n",
    "        )\n",
    "        lime_coefs_runs.append(coefs)\n",
    "\n",
    "    # Convert to NumPy arrays\n",
    "    shap_values_runs = np.array(shap_values_runs)  # shape=(n_runs, p)\n",
    "    lime_coefs_runs = np.array(lime_coefs_runs)    # shape=(n_runs, p)\n",
    "\n",
    "    # Compute feature-wise standard deviation\n",
    "    shap_std = shap_values_runs.std(axis=0)  # shape=(p,)\n",
    "    lime_std = lime_coefs_runs.std(axis=0)   # shape=(p,)\n",
    "\n",
    "    # Accumulate feature-wise standard deviations over 100 samples\n",
    "    shap_std_sum_each_feature += shap_std\n",
    "    lime_std_sum_each_feature += lime_std\n",
    "\n",
    "# At the end, shap_std_sum_each_feature and lime_std_sum_each_feature (shape=(p,))\n",
    "# contain the total sum of std values recorded for each feature across 100 samples.\n",
    "\n",
    "# If Python lists are needed, use .tolist()\n",
    "shap_std_sum_list = shap_std_sum_each_feature.tolist()\n",
    "lime_std_sum_list = lime_std_sum_each_feature.tolist()\n",
    "\n",
    "print(\"각 feature별 SHAP std 합 (길이 p):\", shap_std_sum_list)\n",
    "print(\"각 feature별 LIME std 합 (길이 p):\", lime_std_sum_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
